#!/usr/bin/env python3

import os
import json
import argparse
from pathlib import Path


def load_vulnerability_detection_results(results_dir):
    """Load vulnerability detection results from the single JSON file"""
    model_results = {}
    
    # Load the single vul_detect_score.json file
    vul_file = os.path.join(results_dir, 'vulnerability_detection', 'primevul', 'evaluations', 'vul_detect_score.json')
    
    if os.path.exists(vul_file):
        with open(vul_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
            for model_name, model_data in data.items():
                if 'primevul' in model_data and 'primevul_pair' in model_data:
                    metrics = model_data['primevul']
                    pair_metrics = model_data['primevul_pair']
                    if not pair_metrics:
                        raise ValueError(f"Empty primevul_pair data for model {model_name}")
                    
                    # Validate required keys exist
                    required_keys = ['P-C', 'P-V', 'P-B', 'P-R']
                    missing_keys = [key for key in required_keys if key not in pair_metrics]
                    if missing_keys:
                        raise ValueError(f"Missing primevul_pair keys {missing_keys} for model {model_name}")
                    
                    # Convert to percentages
                    p_c_raw = pair_metrics.get('P-C', 0)
                    p_v_raw = pair_metrics.get('P-V', 0)
                    p_b_raw = pair_metrics.get('P-B', 0)
                    p_r_raw = pair_metrics.get('P-R', 0)
                    
                    # Calculate sum for percentage conversion
                    p_sum = p_c_raw + p_v_raw + p_b_raw + p_r_raw
                    
                    model_results[model_name] = {
                        'accuracy': metrics.get('accuracy', 0) * 100,
                        'precision': metrics.get('precision', 0) * 100,
                        'recall': metrics.get('recall', 0) * 100,
                        'f1': metrics.get('f1', 0) * 100,
                        'p_c': (p_c_raw / p_sum * 100) if p_sum > 0 else 0,
                        'p_v': (p_v_raw / p_sum * 100) if p_sum > 0 else 0,
                        'p_b': (p_b_raw / p_sum * 100) if p_sum > 0 else 0,
                        'p_r': (p_r_raw / p_sum * 100) if p_sum > 0 else 0
                    }
    
    return model_results


def format_score(score, top_scores, metric_key):
    """Format score with appropriate markup for top 3 scores in the column"""
    column_scores = top_scores.get(metric_key, [])
    
    if len(column_scores) >= 1 and abs(score - column_scores[0]) < 0.01:
        return f"\\cellcolor{{firstbg}}\\textcolor{{firsttext}}{{\\textbf{{{score:.1f}}}}}"
    elif len(column_scores) >= 2 and abs(score - column_scores[1]) < 0.01:
        return f"\\cellcolor{{secondbg}}\\textcolor{{secondtext}}{{\\textbf{{{score:.1f}}}}}"
    elif len(column_scores) >= 3 and abs(score - column_scores[2]) < 0.01:
        return f"\\cellcolor{{thirdbg}}\\textcolor{{thirdtext}}{{\\textbf{{{score:.1f}}}}}"
    else:
        return f"{score:.1f}"


def find_top_scores(model_results):
    """Find the top 3 scores for each metric"""
    top_scores = {}
    
    metrics = ['accuracy', 'precision', 'recall', 'f1', 'p_c', 'p_v', 'p_b', 'p_r']
    
    for metric in metrics:
        metric_scores = []
        for model_name, model_data in model_results.items():
            score = model_data.get(metric, 0)
            metric_scores.append(score)
        
        # Sort scores in descending order and take top 3
        metric_scores.sort(reverse=True)
        top_scores[metric] = metric_scores[:3]
    
    return top_scores


def clean_model_name(model_name):
    """Clean up model name for display in table"""
    # Handle common model name patterns
    name_mappings = {
        'claude-3-5-sonnet-20241022': 'Claude-3.5-Sonnet-20241022',
        'claude-3-5-haiku-20241022': 'Claude-3.5-Haiku-20241022',
        'anthropic_claude-3.7-sonnet': 'Claude-3.7-Sonnet',
        'meta-llama_Llama-3.3-70B-Instruct': 'LLaMA-3.3-70B-Instruct',
        'meta-llama_Meta-Llama-3.1-70B-Instruct': 'LLaMA-3.1-70B-Instruct',
        'meta-llama_Meta-Llama-3.1-8B-Instruct': 'LLaMA-3.1-8B-Instruct',
        'meta-llama_Llama-4-Scout-17B-16E-Instruct': 'LLaMA-4-Scout-17B-16E-Instruct',
        'o3-mini': 'o3-mini (Med)',
        'o4-mini': 'o4-mini (Med)',
        'gpt-4o-2024-11-20': 'GPT-4o-2024-11-20',
        'gpt-4-turbo-2024-04-09': 'GPT-4-turbo-2024-04-09',
        'gpt-4.1-2025-04-14': 'GPT-4.1-2025-04-14',
        'gpt-3.5-turbo-0125': 'GPT-3.5-turbo-0125',
        'google_gemini-2.5-pro-preview-05-06': 'Gemini-2.5-Pro-05-06',
        'google_gemma-3-27b-it': 'Gemma-3-27B-Instruct',
        'grok-3-mini-beta': 'Grok-3-Mini (High)',
        'deepseek-chat': 'DeepSeek-V3',
        'deepseek-reasoner': 'DeepSeek-R1',
        'deepseek_deepseek-r1-0528': 'DeepSeek-R1 (0528)',
        'deepseek_deepseek-r1': 'DeepSeek-R1',
        'Qwen_Qwen2.5-72B-Instruct': 'Qwen2.5-72B-Instruct',
        'Qwen_Qwen2.5-Coder-32B-Instruct': 'Qwen2.5-Coder-32B-Instruct',
        'Qwen_Qwen3-235B-A22B': 'Qwen3-235B-A22B',
        'Qwen_Qwen3-30B-A3B': 'Qwen3-30B-A3B',
        'Qwen_Qwen3-32B': 'Qwen3-32B',
        'GPT-5': 'GPT-5'
    }
    
    return name_mappings.get(model_name, model_name)


def create_latex_table(output_path, model_results):
    """Create a new LaTeX table for vulnerability detection results"""
    
    # Find top 3 scores for formatting
    top_scores = find_top_scores(model_results)
    
    # Sort models by F1 score (descending)
    sorted_models = sorted(model_results.items(), 
                          key=lambda x: x[1].get('f1', 0), 
                          reverse=True)
    
    # Generate table content
    content = """\\begin{table*}[ht]
\\centering
\\small
\\setlength{\\tabcolsep}{0.4em}
\\caption{Performance on PRIMEVUL and PRIMEVUL Paired (\\%) by Model}
\\label{tab:vuln_detect}
\\begin{tabular}{@{}c|cccc|cccc@{}}
\\toprule
 & \\multicolumn{4}{c}{\\textbf{PRIMEVUL}} & \\multicolumn{4}{c}{\\textbf{PRIMEVUL Paired}} \\\\ 
\\cmidrule(lr){2-5} \\cmidrule(l){6-9}
\\textbf{Model} 
 & \\textbf{Accuracy} & \\textbf{Precision} & \\textbf{Recall} & \\textbf{F1 Score}
 & \\textbf{P-C} & \\textbf{P-V} & \\textbf{P-B} & \\textbf{P-R} \\\\
\\midrule
"""
    
    # Generate table rows
    for model_name, metrics in sorted_models:
        # Clean up model name for display
        display_name = clean_model_name(model_name)
        
        # Get formatted scores for PRIMEVUL
        accuracy = format_score(metrics.get('accuracy', 0), top_scores, 'accuracy')
        precision = format_score(metrics.get('precision', 0), top_scores, 'precision')
        recall = format_score(metrics.get('recall', 0), top_scores, 'recall')
        f1 = format_score(metrics.get('f1', 0), top_scores, 'f1')
        
        # Get formatted scores for PRIMEVUL Paired
        p_c = format_score(metrics.get('p_c', 0), top_scores, 'p_c')
        p_v = format_score(metrics.get('p_v', 0), top_scores, 'p_v')
        p_b = format_score(metrics.get('p_b', 0), top_scores, 'p_b')
        p_r = format_score(metrics.get('p_r', 0), top_scores, 'p_r')
        
        row = f"{display_name:<35} & {accuracy} & {precision} & {recall} & {f1} & {p_c} & {p_v} & {p_b} & {p_r} \\\\"
        content += row + "\n"
    
    content += """\\bottomrule
\\end{tabular}
\\end{table*}
"""
    
    # Write the table
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(content)
    
    print(f"LaTeX table written to: {output_path}")


def create_prompt_id_multicolumn_table(output_path, model_results_by_prompt):
    """Create one LaTeX table with multicolumns for Prompt ID 1/2/3"""

    # 收集所有出现过的 model_name，保证对齐
    all_models = set()
    for pid in [1, 2, 3]:
        all_models.update(model_results_by_prompt[pid].keys())

    # 对每个 prompt_id 找 top_scores，用于上色
    top_scores_by_pid = {
        pid: find_top_scores(model_results_by_prompt[pid]) for pid in [1, 2, 3]
    }

    # 按 Prompt 1 的 F1 排序，如果没在 Prompt 1 出现就排到后面
    sorted_models = sorted(
        all_models,
        key=lambda m: model_results_by_prompt[1].get(m, {}).get("f1", 0),
        reverse=True,
    )

    metrics = ["accuracy", "precision", "recall", "f1", "p_c", "p_v", "p_b", "p_r"]

    content = """\\begin{table*}[ht]
\\centering
\\small
\\setlength{\\tabcolsep}{0.2em}
\\caption{Performance on PRIMEVUL and PRIMEVUL Paired (\\%) by Model across Prompt IDs}
\\label{tab:vuln_detect_promptid}
\\resizebox{\\textwidth}{!}{%
\\begin{tabular}{@{}c|cccccccc|cccccccc|cccccccc@{}}
\\toprule
 & \\multicolumn{8}{c}{\\textbf{Prompt ID 1}} & \\multicolumn{8}{c}{\\textbf{Prompt ID 2}} & \\multicolumn{8}{c}{\\textbf{Prompt ID 3}} \\\\
\\cmidrule(lr){2-9} \\cmidrule(lr){10-17} \\cmidrule(l){18-25}
\\textbf{Model} 
 & Acc & Prec & Rec & F1 & P-C & P-V & P-B & P-R
 & Acc & Prec & Rec & F1 & P-C & P-V & P-B & P-R
 & Acc & Prec & Rec & F1 & P-C & P-V & P-B & P-R \\\\
\\midrule
"""

    for model_name in sorted_models:
        display_name = clean_model_name(model_name)
        row = display_name

        for pid in [1, 2, 3]:
            metrics_dict = model_results_by_prompt[pid].get(model_name, {})
            for m in metrics:
                score = metrics_dict.get(m, 0)
                row += " & " + format_score(score, top_scores_by_pid[pid], m)

        row += " \\\\"
        content += row + "\n"

    content += """\\bottomrule
\\end{tabular}}
\\end{table*}
"""

    with open(output_path, "w", encoding="utf-8") as f:
        f.write(content)

    print(f"LaTeX multicolumn Prompt ID table written to: {output_path}")

def load_vulnerability_detection_results_by_prompt(results_dir):
    """Load vulnerability detection results grouped by (model_name, prompt_id)"""
    model_results_by_prompt = {1: {}, 2: {}, 3: {}}
    
    vul_file = os.path.join(results_dir, 'vulnerability_detection', 'primevul', 'evaluations', 'vul_detect_score.json')
    
    if os.path.exists(vul_file):
        with open(vul_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            print(data)
            for model_name, entries in data.items():
                # 假设 data[model_name] 是一个 list，每个 entry 有 prompt_id, metrics 等
                if isinstance(entries, list):
                    for entry in entries:
                        pid = entry.get("prompt_id")
                        if pid not in [1, 2, 3]:
                            continue
                        
                        metrics = entry.get("primevul", {})
                        pair_metrics = entry.get("primevul_pair", [{}])
                        pair_metrics = pair_metrics[0] if pair_metrics else {}
                        
                        p_c_raw = pair_metrics.get('P-C', 0)
                        p_v_raw = pair_metrics.get('P-V', 0)
                        p_b_raw = pair_metrics.get('P-B', 0)
                        p_r_raw = pair_metrics.get('P-R', 0)
                        p_sum = p_c_raw + p_v_raw + p_b_raw + p_r_raw
                        
                        model_results_by_prompt[pid][model_name] = {
                            'accuracy': metrics.get('accuracy', 0) * 100,
                            'precision': metrics.get('precision', 0) * 100,
                            'recall': metrics.get('recall', 0) * 100,
                            'f1': metrics.get('f1', 0) * 100,
                            'p_c': (p_c_raw / p_sum * 100) if p_sum > 0 else 0,
                            'p_v': (p_v_raw / p_sum * 100) if p_sum > 0 else 0,
                            'p_b': (p_b_raw / p_sum * 100) if p_sum > 0 else 0,
                            'p_r': (p_r_raw / p_sum * 100) if p_sum > 0 else 0
                        }
                else:
                    # 如果 data[model_name] 不是 list，就 fallback 为整体结果
                    pass
    
    return model_results_by_prompt

def main():
    parser = argparse.ArgumentParser(description='Create vulnerability detection LaTeX table with latest results')
    parser.add_argument('--results-dir', default='results', help='Results directory (default: results)')
    parser.add_argument('--output', default='latex_table/vulnerability_detection.tex', help='Output LaTeX file')
    
    args = parser.parse_args()
    
    # Load and process results
    print("Loading vulnerability detection results...")
    model_results = load_vulnerability_detection_results(args.results_dir)
    
    print(f"Found results for {len(model_results)} models")
    
    # Create the table
    print("Creating LaTeX table...")
    create_latex_table(args.output, model_results)
    model_results_by_prompt = load_vulnerability_detection_results_by_prompt(args.results_dir)
    create_prompt_id_multicolumn_table('latex_table/vulnerability_detection_promptid_multicol.tex', model_results_by_prompt)

    print("Done!")


if __name__ == '__main__':
    main()