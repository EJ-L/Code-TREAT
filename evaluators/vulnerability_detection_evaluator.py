from evaluators.utils.metrics import calc_primevul_score, calc_primevul_pair_score
from evaluators.utils.utils import remove_thinking_tag
from extractors.regex_extraction_utils.vd_utils import handle_response
from typing import List
import json


import os
import json
from typing import List

# assumes these exist elsewhere:
# - handle_response(response, dataset, tolerate=True)
# - calc_primevul_score(gt_list, pred_list)
# - calc_primevul_pair_score(gt_list, pred_list)

def process_pipeline(files: List[str], parsed_subdir_name: str = "parsed", num_response: int = 1) -> None:
    """
    For each input JSONL file:
      - parse each line's responses into `extracted_output`
      - write <stem>_parsed<suffix> to:
          (1) the input file's parent directory, and
          (2) <parent>/<parsed_subdir_name>/
      - group scores by parent directory and write to:
          <parent>/evaluations/vul_detect_score.json
    """
    # Collect scores grouped by parent_dir
    grouped_scores = {}  # { parent_dir: { model_name: {primevul: ..., primevul_pair: ...} } }

    for filename in files:
        # Per-file accumulators (reset each file)
        single_vulnerability_predictions = []
        single_vulnerability_ground_truths = []
        pair_vulnerability_predictions = []
        pair_vulnerability_ground_truths = []

        file_dir = os.path.dirname(filename)            # .../<parent>/<current_dir>
        parent_dir = os.path.dirname(file_dir)          # .../<parent>
        parsed_dir = os.path.join(parent_dir, parsed_subdir_name)
        os.makedirs(parsed_dir, exist_ok=True)

        base_filename = os.path.basename(filename)      # e.g., name.jsonl

        # Outputs
        out_parsed = os.path.join(parsed_dir, base_filename)

        first_line = True
        model_name = "unknown_model"

        with open(filename, "r", encoding="utf-8") as fin, \
             open(out_parsed, "w", encoding="utf-8") as fout_parsed:
            print(filename)
            for line_num, line in enumerate(fin, 1):
                try:
                    data = json.loads(line)

                    if first_line:
                        model_name = data.get("model_name", "unknown_model")
                        first_line = False

                    responses = data.get("response", [])
                    dataset = data.get("dataset", "")

                    if not isinstance(responses, list):
                        responses = [responses]

                    parsed_responses = []
                    for response in responses[:num_response]:
                        result = handle_response(response, dataset, tolerate=True)
                        parsed_responses.append(result)

                        if dataset == "primevul":
                            ground_truth = data.get("ground_truth", -1)
                            # mirrors your original behavior
                            single_vulnerability_ground_truths.extend([ground_truth] * len(responses))
                            single_vulnerability_predictions.append(result)

                        elif dataset == "primevul_pair":
                            ground_truth = data.get("ground_truth", -1)
                            # mirrors your original behavior
                            pair_vulnerability_ground_truths.extend([ground_truth] * len(responses))
                            pair_vulnerability_predictions.append(result)

                    data["extracted_response"] = parsed_responses

                    # mirror writes to both outputs
                    fout_parsed.write(json.dumps(data) + "\n")

                except json.JSONDecodeError:
                    print(f"Warning: Skipping invalid JSON line in {filename} line {line_num}")
                except Exception as e:
                    print(f"Warning: Error processing line in {filename} line {line_num}: {str(e)}")
                    continue

        # Per-file score goes into its parent_dir bucket (by model_name)
        grouped_scores.setdefault(parent_dir, {})
        grouped_scores[parent_dir][model_name] = {
            "primevul":      calc_primevul_score(single_vulnerability_ground_truths, single_vulnerability_predictions),
            "primevul_pair": calc_primevul_pair_score(pair_vulnerability_ground_truths, pair_vulnerability_predictions),
        }

    # Write one score file per parent_dir to <parent>/evaluations/vul_detect_score.json
    for pd, score in grouped_scores.items():
        evaluation_dir = os.path.join(pd, "evaluations")
        os.makedirs(evaluation_dir, exist_ok=True)
        score_path = os.path.join(evaluation_dir, "vul_detect_score.json")
        with open(score_path, "w", encoding="utf-8") as f:
            json.dump(score, f, indent=4)
        print("Evaluation result is written to:", score_path)
