from treat.core.base_runner import BaseTaskRunner, TaskConfig
from typing import Dict, List, Any, Tuple
import traceback
from treat.core.record_manager import RecordManager
from treat.core.template_manager import Template

class VulnerabilityDetectionRunner(BaseTaskRunner):
    """Unified runner for vulnerability detection."""
    
    def __init__(self, config: TaskConfig):
        self.task_name = "vulnerability_detection"
        super().__init__(config)
    
    def get_task_name(self):
        return self.task_name
    
    def create_work_items(self, dataset: List[Any], templates: List[Any], models: List[Any]) -> List[Tuple]:
        work_items: List[Tuple] = []
        for data in dataset:
            for model in models:
                for template in templates:
                    work_items.append((data, model, template))
        return work_items
    
    
    def process_work_item(self, work_item: Tuple) -> Dict[str, Any]:
        data, model, template = work_item
        model_name = getattr(model, "model_name", "unknown_model")
        
        try:
            ref_key = RecordManager.make_ref_key(self.config.dataset, model_name, data.key, data.idx, self.config.template_categories, template.template_id)
            
            if ref_key is None:
                print(f"ðŸ”§ [DEBUG] Skipping: no_prompt_id")
                return {"status": "skipped", "reason": "no_prompt_id"}

            print(f"ðŸ”§ [DEBUG] Checking if already processed...")
            if self.record_manager.is_ref_processed(ref_key):
                print(f"ðŸ”§ [DEBUG] Skipping: already_processed")
                return {"status": "skipped", "reason": "already_processed"}

            # Build prompt
            print(f"ðŸ”§ [DEBUG] Building prompt...")
            message, wrapped_text = self._build_prompt(data, model, template)
            print(f"ðŸ”§ [DEBUG] Prompt built successfully, message has {len(message)} parts")
            
            # Fetch responses
            print(f"ðŸ”§ [DEBUG] Making API call to {model_name}...")
            response_list = self.fetch_responses_parallel(
                model, message, self.config.n_requests
            )
            print(f"ðŸ”§ [DEBUG] API call completed, got {len(response_list)} responses")

            prompt_id = getattr(template, "template_id", None)

            print(f"ðŸ”§ [DEBUG] Building result object...")
            result = {
                "task": self.task_name,
                "ref_key": ref_key,
                "lang": self.config.language,
                "dataset": data.dataset_name,
                "key": data.key,
                "data_idx": data.idx,
                "prompting_category": template.category,
                "prompt_id": prompt_id,
                "prompt_template": template.template_string,
                "wrapped_text": wrapped_text,
                "model_name": model_name,
                "response": response_list,
            }

            self.record_manager.save_record(model_name, result)
            return {"status": "completed"}

        except Exception as e:
            traceback.print_exc()
            return {"status": "error", "error": str(e)}
        
    def _build_prompt(
        self,
        data: Any,
        model: Any,
        template: Template
    ) -> Tuple[Any, str]:
        if self.config.dataset == "primevul":
            template_info = {
                "code": data.func
            }
        elif self.config.dataset == "primevul_pairs" or self.config.dataset == "primevul_pair":
            template_info = {
                "code1": data.data1.func,
                "code2": data.data2.func,
            }
        else:
            # Default case - try single code format first
            if hasattr(data, 'func'):
                template_info = {
                    "code": data.func
                }
            elif hasattr(data, 'data1') and hasattr(data, 'data2'):
                template_info = {
                    "code1": data.data1.func,
                    "code2": data.data2.func,
                }
            else:
                raise ValueError(f"Unsupported dataset format for dataset: {self.config.dataset}")
        
        result = template.get_prompt(model=model, **template_info)
        return result